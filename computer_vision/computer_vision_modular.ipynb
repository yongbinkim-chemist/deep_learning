{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNO44xJsbHqKZBEBavLE3fc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"adecc150c2a0496aa80f326936719f47":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ab74dbd5b9c84c73a6d8c98e6d3d71a7","IPY_MODEL_f31e0ae7013f472c9154c7001d97d056","IPY_MODEL_928ad7891094401a8d5ee88e08c4d4c8"],"layout":"IPY_MODEL_ac6b378091574ae486f30a8b3ca73374"}},"ab74dbd5b9c84c73a6d8c98e6d3d71a7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6306bf25d59e48b8a3ee15da30bbf3e4","placeholder":"​","style":"IPY_MODEL_83f7ab590e32440eb4fab0eaad81af50","value":"100%"}},"f31e0ae7013f472c9154c7001d97d056":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f3f5470f11944822a830b07afbd7dfcb","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4f2559e6e5c74a8b89638bc02e17a65b","value":5}},"928ad7891094401a8d5ee88e08c4d4c8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c90ff6d2c84146a9894b8e37f99eedac","placeholder":"​","style":"IPY_MODEL_659797198bdd4b4d84e19871c90a6704","value":" 5/5 [23:24&lt;00:00, 285.80s/it]"}},"ac6b378091574ae486f30a8b3ca73374":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6306bf25d59e48b8a3ee15da30bbf3e4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83f7ab590e32440eb4fab0eaad81af50":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f3f5470f11944822a830b07afbd7dfcb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f2559e6e5c74a8b89638bc02e17a65b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c90ff6d2c84146a9894b8e37f99eedac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"659797198bdd4b4d84e19871c90a6704":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# 1. Get Data"],"metadata":{"id":"toK5b9fN0LVS"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VAKWwtmOzy23","executionInfo":{"status":"ok","timestamp":1758377812172,"user_tz":240,"elapsed":879,"user":{"displayName":"yb","userId":"00861961798201398250"}},"outputId":"a4e868f6-fb68-451f-debe-487da70f18ba"},"outputs":[{"output_type":"stream","name":"stdout","text":["data/food-101-mini directory exists.\n","Downloading subset of food-101-mini data...\n","Unzipping food-101-mini data...\n"]}],"source":["import requests\n","import zipfile\n","from pathlib import Path\n","\n","# Setup path to a data folder\n","data_path = Path(\"data/\")\n","image_path = data_path / \"food-101-mini\"\n","\n","if image_path.is_dir():\n","  print(f\"{image_path} directory exists.\")\n","else:\n","  print(f\"Did not find {image_path} directory, creating one...\")\n","  image_path.mkdir(parents=True, exist_ok=True)\n","\n","# Download data\n","with open(image_path / \"food-101-mini.zip\", \"wb\") as f:\n","  request = requests.get(\"https://github.com/yongbinkim-chemist/deep_learning/raw/refs/heads/main/computer_vision/data/food-101-mini/food-101-mini.zip\")\n","  print(\"Downloading subset of food-101-mini data...\")\n","  f.write(request.content)\n","\n","# Unzip data\n","with zipfile.ZipFile(image_path / \"food-101-mini.zip\", \"r\") as zip_ref:\n","  print(\"Unzipping food-101-mini data...\")\n","  zip_ref.extractall(image_path)"]},{"cell_type":"code","source":["!rm -rf data/food-101-mini/__MACOSX/"],"metadata":{"id":"Zk9tcqo6e8bf","executionInfo":{"status":"ok","timestamp":1758377812316,"user_tz":240,"elapsed":139,"user":{"displayName":"yb","userId":"00861961798201398250"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["import os\n","os.makedirs(\"src\", exist_ok=True)"],"metadata":{"id":"8R8k819YJvkW","executionInfo":{"status":"ok","timestamp":1758377812324,"user_tz":240,"elapsed":2,"user":{"displayName":"yb","userId":"00861961798201398250"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# 2. Create DataLoaders"],"metadata":{"id":"MC10LcTOhgIr"}},{"cell_type":"code","source":["%%writefile src/data_setup.py\n","from typing import Tuple, List\n","import torch\n","from torch.utils.data import Subset, DataLoader\n","from torchvision import datasets, transforms\n","from sklearn.model_selection import train_test_split\n","\n","def create_dataloaders(image_path: str,\n","                       transform: transforms.Compose,\n","                       batch_size: int,\n","                       num_workers: int = 1,\n","                       seed: int = None) -> Tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader, List[str]]:\n","  \"\"\"Creates training and testing DataLoaders\n","\n","  Takes in image path and turns it into PyTorch DataLoader.\n","\n","  Args:\n","    image_path: Path to images.\n","    transform: torchvision transforms to perform on images.\n","    batch_size: Number of samples per batch in each of the DataLoaders.\n","    num_workers: An integer for number of workers per DataLoader.\n","    seed: Random seed for train_test_split.\n","\n","  Returns:\n","    A tuple of (train_dataloader, test_dataloader, class_names).\n","\n","  Example usage:\n","    train_dataloader, test_dataloader, class_names = create_dataloaders(image_path=path/to/images\n","                                                                        transform=some_transforms,\n","                                                                        batch_size=32,\n","                                                                        num_workers=1,\n","                                                                        seed=42)\n","  \"\"\"\n","\n","  dataset = datasets.ImageFolder(root=image_path,\n","                                 transform=transform)\n","  class_names = dataset.classes\n","\n","  targets = [label for _, label in dataset.samples]\n","  indices = list(range(len(targets)))\n","  train_indices, test_indices = train_test_split(indices,\n","                                                test_size=0.25,\n","                                                stratify=targets,\n","                                                random_state=seed)\n","\n","  train_dataset = Subset(dataset, train_indices)\n","  test_dataset = Subset(dataset, test_indices)\n","\n","  train_dataloader = DataLoader(dataset=train_dataset,\n","                                batch_size=batch_size,\n","                                shuffle=True,\n","                                num_workers=num_workers,\n","                                pin_memory=True)\n","\n","  test_dataloader = DataLoader(dataset=test_dataset,\n","                              batch_size=batch_size,\n","                              shuffle=False,\n","                              num_workers=num_workers)\n","\n","  return train_dataloader, test_dataloader, class_names\n","\n","if __name__ == \"__main__\":\n","  import os\n","\n","  image_path = \"data/food-101-mini\"\n","  BATCH_SIZE = 32\n","  NUM_WORKERS = os.cpu_count()\n","  SEED = 42\n","  data_transforms = transforms.Compose([transforms.Resize((224, 224)),\n","                                        transforms.ToTensor()])\n","\n","  train_dataloader, test_dataloader, class_names = create_dataloaders(image_path=image_path,\n","                                                                      transform=data_transforms,\n","                                                                      batch_size=BATCH_SIZE,\n","                                                                      num_workers=NUM_WORKERS,\n","                                                                      seed=SEED)\n","  print(train_dataloader, test_dataloader, class_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a9ZruFb4-8Z9","executionInfo":{"status":"ok","timestamp":1758378429564,"user_tz":240,"elapsed":5,"user":{"displayName":"yb","userId":"00861961798201398250"}},"outputId":"8a671bcb-058c-4808-f2cf-e83f3ea684d2"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting src/data_setup.py\n"]}]},{"cell_type":"code","source":["!python src/data_setup.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"quUzm_1RGgNK","executionInfo":{"status":"ok","timestamp":1758378438713,"user_tz":240,"elapsed":7535,"user":{"displayName":"yb","userId":"00861961798201398250"}},"outputId":"a1e35219-3b91-4352-f3cb-7ffa08e953c4"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["<torch.utils.data.dataloader.DataLoader object at 0x7f4474353d70> <torch.utils.data.dataloader.DataLoader object at 0x7f446e2fbe00> ['bibimbap', 'hamburger', 'sashimi']\n"]}]},{"cell_type":"code","source":["from torchvision import transforms\n","from src import data_setup\n","\n","BATCH_SIZE = 32\n","NUM_WORKERS = os.cpu_count()\n","SEED = 42\n","data_transforms = transforms.Compose([transforms.Resize((224, 224)),\n","                                      transforms.ToTensor()])\n","\n","train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(image_path=image_path,\n","                                                                               transform=data_transforms,\n","                                                                               batch_size=BATCH_SIZE,\n","                                                                               num_workers=NUM_WORKERS,\n","                                                                               seed=SEED)\n","\n","train_dataloader, test_dataloader, class_names"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HsW88bVuKELS","executionInfo":{"status":"ok","timestamp":1758378438765,"user_tz":240,"elapsed":50,"user":{"displayName":"yb","userId":"00861961798201398250"}},"outputId":"3d526b9f-3f1d-4da3-a0f9-e0012a981d9a"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<torch.utils.data.dataloader.DataLoader at 0x7ce678b1af00>,\n"," <torch.utils.data.dataloader.DataLoader at 0x7ce67943c380>,\n"," ['bibimbap', 'hamburger', 'sashimi'])"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["# 3. Create Models\n"],"metadata":{"id":"HCVj2JVWhnOr"}},{"cell_type":"markdown","source":["## 3.1 TinyVGGNet"],"metadata":{"id":"7YwfUMh4iaRq"}},{"cell_type":"code","source":["%%writefile src/models.py\n","\n","import torch\n","from torch import nn\n","\n","class VGGNet(nn.Module):\n","  # cfgs = { \"A\": [64, \"M\", 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\n","  #         \"B\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\n","  #         \"D\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, 256, \"M\", 512, 512, 512, \"M\", 512, 512, 512, \"M\"],\n","  #         \"E\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, 256, 256, \"M\", 512, 512, 512, 512, \"M\", 512, 512, 512, 512, \"M\"] }\n","\n","  def __init__(self,\n","               num_classes: int,\n","               drop_p: float = 0.5,\n","               batch_norm: bool = False,\n","               init_weights: bool = True):\n","\n","    super().__init__()\n","    self.cfg = [64, \"M\", 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"]\n","\n","    self.features = self.create_conv_layers(batch_norm=batch_norm)\n","    self.avgpool = nn.AdaptiveAvgPool2d(output_size=(7, 7))\n","    self.classifier = nn.Sequential(nn.Flatten(),\n","                                    nn.Linear(in_features=512 * 7 * 7,\n","                                              out_features=4096,\n","                                              bias=True),\n","                                    nn.ReLU(inplace=True), # makes nn.ReLU() overwirte the input tensor directly instead of creating a new one\n","                                    nn.Dropout(p=drop_p),\n","                                    nn.Linear(in_features=4096,\n","                                              out_features=4096,\n","                                              bias=True),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Dropout(p=drop_p),\n","                                    nn.Linear(in_features=4096,\n","                                              out_features=num_classes,\n","                                              bias=True))\n","\n","    if init_weights:\n","      # `self.modules()` returns the model and all submodules (layers, activations, etc..) recursively, while self.children() only returns the direct child modules.\n","      # It's commonly used to loop over layers for tasks like weight initialization.\n","      for m in self.modules():\n","        if isinstance(m, nn.Conv2d):\n","          nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n","          if m.bias is not None:\n","            nn.init.constant_(m.bias, val=0)\n","        elif isinstance(m, nn.Linear):\n","          nn.init.normal_(m.weight, mean=0, std=0.01)\n","          nn.init.constant_(m.bias, val=0)\n","\n","  def forward(self, x: torch.Tensor) -> torch.Tensor:\n","    return self.classifier(self.avgpool(self.features(x)))\n","\n","  def create_conv_layers(self, batch_norm: bool):\n","    layers = []\n","    in_channels = 3 # color channels\n","\n","    for x in self.cfg:\n","      if type(x) == int:\n","        out_channels = x\n","\n","        if batch_norm:\n","          layers += [nn.Conv2d(in_channels=in_channels,\n","                               out_channels=out_channels,\n","                               kernel_size=3,\n","                               stride=1,\n","                               padding=1,\n","                               bias=False),\n","                     nn.BatchNorm2d(num_features=out_channels),\n","                     nn.ReLU()]\n","        else:\n","          layers += [nn.Conv2d(in_channels=in_channels,\n","                               out_channels=out_channels,\n","                               kernel_size=3,\n","                               stride=1,\n","                               padding=1),\n","                     nn.ReLU()]\n","        in_channels = x\n","      else:\n","        layers += [nn.MaxPool2d(kernel_size=2,\n","                                stride=2,\n","                                padding=0)]\n","\n","    return nn.Sequential(*layers)\n","\n","if __name__ == \"__main__\":\n","  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","  model = VGGNet(num_classes=3,\n","                 drop_p=0.5,\n","                 batch_norm=True,\n","                 init_weights=True).to(device)\n","  print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Be55GZwKq7H","executionInfo":{"status":"ok","timestamp":1758384299764,"user_tz":240,"elapsed":12,"user":{"displayName":"yb","userId":"00861961798201398250"}},"outputId":"a9fbe3ce-7deb-41f1-f39c-1f7c5f59f367"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting src/models.py\n"]}]},{"cell_type":"code","source":["!python src/models.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lKcESPwyjPtZ","executionInfo":{"status":"ok","timestamp":1758384269535,"user_tz":240,"elapsed":4527,"user":{"displayName":"yb","userId":"00861961798201398250"}},"outputId":"04415397-0082-40a9-ba84-1b640acff9ff"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["VGGNet(\n","  (features): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (6): ReLU()\n","    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (10): ReLU()\n","    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (13): ReLU()\n","    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (17): ReLU()\n","    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (20): ReLU()\n","    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (24): ReLU()\n","    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (27): ReLU()\n","    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n","  (classifier): Sequential(\n","    (0): Flatten(start_dim=1, end_dim=-1)\n","    (1): Linear(in_features=25088, out_features=4096, bias=True)\n","    (2): ReLU(inplace=True)\n","    (3): Dropout(p=0.5, inplace=False)\n","    (4): Linear(in_features=4096, out_features=4096, bias=True)\n","    (5): ReLU(inplace=True)\n","    (6): Dropout(p=0.5, inplace=False)\n","    (7): Linear(in_features=4096, out_features=3, bias=True)\n","  )\n",")\n"]}]},{"cell_type":"code","source":["from src import models\n","\n","model = models.VGGNet(num_classes=len(class_names),\n","                      drop_p=0.5,\n","                      batch_norm=True,\n","                      init_weights=True)\n","\n","model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RY3JviM-3bT3","executionInfo":{"status":"ok","timestamp":1758384342002,"user_tz":240,"elapsed":2088,"user":{"displayName":"yb","userId":"00861961798201398250"}},"outputId":"e9543b18-4639-4e1b-dd73-858c22cb87f1"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["VGGNet(\n","  (features): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (6): ReLU()\n","    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (10): ReLU()\n","    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (13): ReLU()\n","    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (17): ReLU()\n","    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (20): ReLU()\n","    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (24): ReLU()\n","    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (27): ReLU()\n","    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n","  (classifier): Sequential(\n","    (0): Flatten(start_dim=1, end_dim=-1)\n","    (1): Linear(in_features=25088, out_features=4096, bias=True)\n","    (2): ReLU(inplace=True)\n","    (3): Dropout(p=0.5, inplace=False)\n","    (4): Linear(in_features=4096, out_features=4096, bias=True)\n","    (5): ReLU(inplace=True)\n","    (6): Dropout(p=0.5, inplace=False)\n","    (7): Linear(in_features=4096, out_features=3, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["# 4. Training and testing loops + train function"],"metadata":{"id":"AbY4rWa59KGs"}},{"cell_type":"code","source":["%%writefile src/engine.py\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from tqdm.auto import tqdm\n","from timeit import default_timer as timer\n","from typing import Dict, List, Tuple\n","\n","def train_step(model: nn.Module,\n","               train_dataloader: torch.utils.data.DataLoader,\n","               optimizer: torch.optim.Optimizer,\n","               loss_fn: nn.Module = nn.CrossEntropyLoss(),\n","               device: str = \"cpu\") -> Tuple[float, float]:\n","\n","  train_loss, train_acc = 0, 0\n","  model.train()\n","\n","  for batch, (X, y) in enumerate(train_dataloader):\n","    X, y = X.to(device), y.to(device)\n","    # Forward pass\n","    y_logits = model(X)\n","    y_preds = torch.softmax(y_logits, dim=1).argmax(dim=1)\n","    # Calculate the loss\n","    loss = loss_fn(y_logits, y)\n","    train_loss += loss.item()\n","    train_acc += (y_preds == y).sum().item()/len(y_preds)\n","    # Optimizer zero grad\n","    optimizer.zero_grad()\n","    # Loss backward (backpropagation)\n","    loss.backward()\n","    # Optimizer step\n","    optimizer.step()\n","\n","  train_loss /= len(train_dataloader)\n","  train_acc /= len(train_dataloader)\n","  return train_loss, train_acc\n","\n","def test_step(model: nn.Module,\n","              test_dataloader: torch.utils.data.DataLoader,\n","              loss_fn: nn.Module = nn.CrossEntropyLoss(),\n","              device: str = \"cpu\") -> Tuple[float, float]:\n","\n","  test_loss, test_acc = 0, 0\n","  model.eval()\n","\n","  with torch.inference_mode():\n","    for batch, (X, y) in enumerate(test_dataloader):\n","      X, y = X.to(device), y.to(device)\n","      # Forward pass\n","      y_logits = model(X)\n","      # Calculate the loss\n","      loss = loss_fn(y_logits, y)\n","      test_loss += loss.item()\n","      test_acc += (y_logits.argmax(dim=1) == y).sum().item()/len(y_logits)\n","\n","  test_loss /= len(test_dataloader)\n","  test_acc /= len(test_dataloader)\n","  return test_loss, test_acc\n","\n","def train(model: nn.Module,\n","          train_dataloader: torch.utils.data.DataLoader,\n","          test_dataloader: torch.utils.data.DataLoader,\n","          optimizer: torch.optim.Optimizer,\n","          loss_fn: nn.Module = nn.CrossEntropyLoss(),\n","          epochs: int = 5,\n","          device: str = \"cpu\") -> Dict[str, List]:\n","\n","  results = {\"train_loss\": [],\n","             \"train_acc\": [],\n","             \"test_loss\": [],\n","             \"test_acc\": []}\n","\n","  start_time = timer()\n","  for epoch in tqdm(range(epochs)):\n","    train_loss, train_acc = train_step(model=model,\n","                                       train_dataloader=train_dataloader,\n","                                       optimizer=optimizer,\n","                                       loss_fn=loss_fn,\n","                                       device=device)\n","    test_loss, test_acc = test_step(model=model,\n","                                    test_dataloader=test_dataloader,\n","                                    loss_fn=loss_fn,\n","                                    device=device)\n","    print(f\"Epoch: {epoch+1} | Train loss: {train_loss:.4f} | Train acc: {train_acc:.4f} | Test loss: {test_loss:.4f} | Test acc: {test_acc:.4f}\")\n","    results[\"train_loss\"].append(train_loss)\n","    results[\"train_acc\"].append(train_acc)\n","    results[\"test_loss\"].append(test_loss)\n","    results[\"test_acc\"].append(test_acc)\n","\n","  return results\n","\n","if __name__ == \"__main__\":\n","  import os\n","  import torch\n","  from torch import nn\n","  from torchvision import transforms\n","  import data_setup, models\n","\n","  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","  image_path = \"data/food-101-mini\"\n","  BATCH_SIZE = 32\n","  NUM_WORKERS = os.cpu_count()\n","  SEED = 42\n","  EPOCHS = 5\n","  LR = 0.001\n","\n","  data_transforms = transforms.Compose([transforms.Resize((64, 64)),\n","                                        transforms.ToTensor()])\n","\n","  train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(image_path=image_path,\n","                                                                                 transform=data_transforms,\n","                                                                                 batch_size=BATCH_SIZE,\n","                                                                                 num_workers=NUM_WORKERS,\n","                                                                                 seed=SEED)\n","  model = models.VGGNet(num_classes=len(class_names),\n","                        drop_p=0.5,\n","                        batch_norm=True,\n","                        init_weights=True).to(device)\n","\n","  loss_fn = nn.CrossEntropyLoss()\n","  optimizer = torch.optim.Adam(params=model.parameters(),\n","                               lr=LR)\n","  results = train(model=model,\n","                  train_dataloader=train_dataloader,\n","                  test_dataloader=test_dataloader,\n","                  optimizer=optimizer,\n","                  loss_fn=loss_fn,\n","                  epochs=EPOCHS,\n","                  device=device)\n","  print(results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qMXo0U0Q37tf","executionInfo":{"status":"ok","timestamp":1758387293684,"user_tz":240,"elapsed":7,"user":{"displayName":"yb","userId":"00861961798201398250"}},"outputId":"45cf67df-dda2-4037-8016-b5de24333981"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting src/engine.py\n"]}]},{"cell_type":"code","source":["# !python src/engine.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z3RCUkf_BQQp","executionInfo":{"status":"ok","timestamp":1758387518335,"user_tz":240,"elapsed":223981,"user":{"displayName":"yb","userId":"00861961798201398250"}},"outputId":"021aa8b0-ba6e-4f37-c018-ac9491dc309d"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["\r  0% 0/5 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n","  warnings.warn(warn_msg)\n","Epoch: 1 | Train loss: 45.3237 | Train acc: 0.4141 | Test loss: 2.2816 | Test acc: 0.3201\n"," 20% 1/5 [00:49<03:17, 49.37s/it]Epoch: 2 | Train loss: 2.6643 | Train acc: 0.4492 | Test loss: 1.1600 | Test acc: 0.3400\n"," 40% 2/5 [01:31<02:14, 44.83s/it]Epoch: 3 | Train loss: 1.9323 | Train acc: 0.3398 | Test loss: 1.2563 | Test acc: 0.3400\n"," 60% 3/5 [02:10<01:24, 42.31s/it]Epoch: 4 | Train loss: 1.6414 | Train acc: 0.3672 | Test loss: 1.1743 | Test acc: 0.3201\n"," 80% 4/5 [02:49<00:41, 41.07s/it]Epoch: 5 | Train loss: 1.4105 | Train acc: 0.3594 | Test loss: 1.0313 | Test acc: 0.3400\n","100% 5/5 [03:33<00:00, 42.77s/it]\n","{'train_loss': [45.3236963422969, 2.664296138274949, 1.9323353916406631, 1.6414407938718796, 1.4105334728956223], 'train_acc': [0.4140625, 0.44921875, 0.33984375, 0.3671875, 0.359375], 'test_loss': [2.281573216120402, 1.160037676493327, 1.2562576532363892, 1.1742773453394573, 1.031334638595581], 'test_acc': [0.32007575757575757, 0.33996212121212127, 0.33996212121212127, 0.32007575757575757, 0.33996212121212127]}\n"]}]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from src import engine\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","EPOCHS = 5\n","LR = 0.001\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(params=model.parameters(),\n","                              lr=LR)\n","results = engine.train(model=model,\n","                       train_dataloader=train_dataloader,\n","                       test_dataloader=test_dataloader,\n","                       optimizer=optimizer,\n","                       loss_fn=loss_fn,\n","                       epochs=EPOCHS,\n","                       device=device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":188,"referenced_widgets":["adecc150c2a0496aa80f326936719f47","ab74dbd5b9c84c73a6d8c98e6d3d71a7","f31e0ae7013f472c9154c7001d97d056","928ad7891094401a8d5ee88e08c4d4c8","ac6b378091574ae486f30a8b3ca73374","6306bf25d59e48b8a3ee15da30bbf3e4","83f7ab590e32440eb4fab0eaad81af50","f3f5470f11944822a830b07afbd7dfcb","4f2559e6e5c74a8b89638bc02e17a65b","c90ff6d2c84146a9894b8e37f99eedac","659797198bdd4b4d84e19871c90a6704"]},"id":"zjZ2a4x0CwK8","executionInfo":{"status":"ok","timestamp":1758388990549,"user_tz":240,"elapsed":1404951,"user":{"displayName":"yb","userId":"00861961798201398250"}},"outputId":"cadfd1a4-d770-4fda-e530-65834287861a"},"execution_count":31,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/5 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adecc150c2a0496aa80f326936719f47"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n","  warnings.warn(warn_msg)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 1 | Train loss: 36.4322 | Train acc: 0.3164 | Test loss: 2.2318 | Test acc: 0.3400\n","Epoch: 2 | Train loss: 2.6220 | Train acc: 0.4258 | Test loss: 1.1033 | Test acc: 0.3400\n","Epoch: 3 | Train loss: 1.2352 | Train acc: 0.4375 | Test loss: 1.1803 | Test acc: 0.3201\n","Epoch: 4 | Train loss: 1.4826 | Train acc: 0.3359 | Test loss: 1.3212 | Test acc: 0.3201\n","Epoch: 5 | Train loss: 1.1533 | Train acc: 0.4531 | Test loss: 1.1641 | Test acc: 0.3400\n"]}]},{"cell_type":"markdown","source":["# 5. Function to save the model"],"metadata":{"id":"O1wxEb0aISDM"}},{"cell_type":"code","source":["%%writefile src/utils.py\n","\n","import torch\n","from torch import nn\n","from pathlib import Path\n","\n","def save_model(model: nn.Module,\n","               target_dir: str,\n","               model_name: str) -> None:\n","\n","  \"\"\"Saves a PyTorch model to a target directory.\n","\n","  Args:\n","    model: A target PyTorch model to save.\n","    target_dir: A directory for saving the model to.\n","    model_name: A filename for the saved model (.pth or .pt).\n","\n","  Example usage:\n","    save_model(model=model,\n","               target_dir=\"models\",\n","               model_name=\"computer_vision_modular.pth\")\n","  \"\"\"\n","\n","  target_dir_path = Path(target_dir)\n","  target_dir_path.mkdir(parents=True,\n","                        exist_ok=True)\n","\n","  assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n","  model_save_path = target_dir_path / model_name\n","\n","  print(f\"[INFO] Saving model to: {model_save_path}\")\n","  torch.save(obj=model.state_dict(),\n","             f=model_save_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_r06BbuiENEq","executionInfo":{"status":"ok","timestamp":1758388990897,"user_tz":240,"elapsed":70,"user":{"displayName":"yb","userId":"00861961798201398250"}},"outputId":"8303211a-824e-4e43-aa97-fc34c68dfbd2"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing src/utils.py\n"]}]},{"cell_type":"code","source":["from src import utils\n","\n","utils.save_model(model=model,\n","                 target_dir=\"models\",\n","                 model_name=\"computer_vision_modular.pth\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ok6uEmFGJLmU","executionInfo":{"status":"ok","timestamp":1758388996910,"user_tz":240,"elapsed":6015,"user":{"displayName":"yb","userId":"00861961798201398250"}},"outputId":"ba896a48-8853-4867-db7e-d7b2bf669f6a"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] Saving model to: models/computer_vision_modular.pth\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"_1ywva4kJYdA"},"execution_count":null,"outputs":[]}]}